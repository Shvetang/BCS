{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42652ce-7cbb-4372-960f-885d8929bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK 2: Image Deblurring Using U-Net architecture\n",
    "\n",
    "Objective: The objective of this task is to develop a deep learning model to perform image deblurring and denoising using the UNet architecture. The provided dataset consists of triplets of images for each scene: sharp, defocused-blurred, and motion-blurred images. The model should be capable of taking a blurred image as input and generating a high-quality, sharp, and noise-free image as output.\n",
    "\n",
    "PS: The performance of the model will be evaluated based on the L2 norm distance between the predicted sharp image and the ground truth sharp image on both the training and test sets. Build and train a UNet-based model to deblur and denoise the defocused-blurred and motion-blurred images, aiming to restore them to their corresponding sharp versions. Submission should be only in PyTorch framework.\n",
    "\n",
    "Deadline: As the secy tasks have been released, deadline for this task is 25th May EOD.\n",
    "\n",
    "Link To the Dataset: https://www.kaggle.com/datasets/kwentar/blur-dataset\n",
    "\n",
    "Understanding UNet: https://paperswithcode.com/method/u-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2a0143-3b3d-4569-91bf-728945e7c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab540a7-cd0e-4754-92f8-b862b624c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp=[]\n",
    "defocused_blurred=[]\n",
    "motion_blurred=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddee9c5-7eb1-4197-a880-5125bdb4abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_images=sorted(os.listdir(\"archive/sharp\"))\n",
    "defocused_blurred_images=sorted(os.listdir(\"archive/defocused_blurred\"))\n",
    "motion_blurred_images=sorted(os.listdir(\"archive/motion_blurred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e793507c-8382-42a4-a5e8-867c031b102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in sharp_images:\n",
    "    img=cv2.imread(\"archive/sharp/\"+img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    if img.shape[1]>img.shape[0]:\n",
    "        img=cv2.transpose(img)\n",
    "    img=cv2.resize(img,(352,528))\n",
    "    img=np.array(img)\n",
    "    sharp.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d64ba7-f876-4e95-8fc5-41cb2d79bc86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in defocused_blurred_images:\n",
    "    img=cv2.imread(\"archive/defocused_blurred/\"+img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    if img.shape[1]>img.shape[0]:\n",
    "        img=cv2.transpose(img)\n",
    "    img=cv2.resize(img,(352,528))\n",
    "    img=np.array(img)\n",
    "    defocused_blurred.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64e2485-8ccf-42e5-ba2d-77e97daf9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in motion_blurred_images:\n",
    "    img=cv2.imread(\"archive/motion_blurred/\"+img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    if img.shape[1]>img.shape[0]:\n",
    "        img=cv2.transpose(img)\n",
    "    img=cv2.resize(img,(352,528))\n",
    "    img=np.array(img)\n",
    "    motion_blurred.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a195574-e8a7-4cf1-ba75-5c72245925ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 350 350\n"
     ]
    }
   ],
   "source": [
    "print(len(sharp),len(motion_blurred),len(defocused_blurred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508ba321-5413-455d-943f-f762e8ff985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp=np.array(sharp)\n",
    "defocused_blurred=np.array(defocused_blurred)\n",
    "motion_blurred=np.array(motion_blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e97498-1b5d-40de-8931-35a556d733c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp=sharp/255.0\n",
    "defocused_blurred=defocused_blurred/255.0\n",
    "motion_blurred=motion_blurred/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d43946d-9a22-46c8-9748-1c3eb3c2caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)  # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a1e61b-5cc9-4013-8227-f1f46e93312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018a5af7-c536-4318-a4e7-372580915362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images1, val_images, train_masks1, val_masks = train_test_split(defocused_blurred, sharp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92d0782-e9f3-4609-b1c0-dcaa62d7dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageMaskDataset(train_images1, train_masks1)\n",
    "val_dataset = ImageMaskDataset(val_images, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3b7fd16-1040-41df-9fe3-279500de740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "criterion = nn.MSELoss()                                    #L2 Criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a979c-b63f-4c11-9985-8ff3aca1d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.permute(0, 3, 1, 2))\n",
    "        loss = criterion(outputs, masks.permute(0, 3, 1, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            outputs = model(images.permute(0, 3, 1, 2))\n",
    "            loss = criterion(outputs, masks.permute(0, 3, 1, 2))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9477a3dd-f345-4dcb-b7d2-9c422d29d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images2, val_images, train_masks2, val_masks = train_test_split(motion_blurred, sharp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba73d7b-a4f8-49f4-a71f-be0d062c4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageMaskDataset(train_images2, train_masks2)\n",
    "val_dataset = ImageMaskDataset(val_images, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca5bad-caf7-45da-be01-7c1b5a903502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
